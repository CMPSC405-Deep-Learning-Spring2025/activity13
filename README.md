# Activity 13
## Due: 9am on March 19, 2025

## Objectives
- Understand the basics of word embeddings and how they capture semantic meaning.
- Compare unigram, bigram, and trigram models to see how context length affects prediction.
- Practice modifying existing Python scripts to deepen understanding of NLP workflows.

## Tasks
1. In `WordEmbed.py`, experiment with different vector sizes and windows, then analyze the most similar words to a chosen keyword. Comment on any surprising similarities.
2. In `Sequence.py`, adjust your n-gram models to generate or predict several words. Document how unigram, bigram, and trigram differ in accuracy and context awareness.
3. Summarize your key findings in the "Observations" section below, focusing on how embeddings differ from simple n-gram approaches.

## Observations
TODO: for each task above describe your observations 
1. TODO
2. TODO
3. TODO

## Assessment

To receive a point for this activity, submit a modified README with your answers under "Observations" and modified `Sequence.py` and `WordEmbed.py` with comments and modifications from the tasks completed.
